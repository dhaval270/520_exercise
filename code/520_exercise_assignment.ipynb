{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGcZm13-47Nx",
        "outputId": "218ce16e-47cb-4694-f8a0-a095fefdd640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/135.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/98.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets groq together transformers pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyAErPty4-iu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from datetime import datetime\n",
        "from groq import Groq\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxX7X8gB5Qvt"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = \"your-api-key\"\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "304f7423f0f3423289886e185f789c80",
            "745c8c4199584d6db45742f2df5e3354",
            "f71de3ebaa7d4ad1b08da5bf4c4aee0c",
            "91c1ad2e26194be49f425cb654b1223e",
            "1ecdc64d79d9439c8f69e0d8a197a6f0",
            "e3f563213891414898392c48e761c8c0",
            "19b476a7339043afa9f46073346919d3",
            "5ef82c01953e426ba68d41835ff72aeb",
            "905ed85b9b4c483b9068aa39a7b7cd38",
            "5902ead2046a43daa2690679a0b125ca",
            "10bb747a1f4f4a11a448cdfea2ae3709",
            "cbc47e20a0704c00a2bdde345fc037e3",
            "298d610d241e4d4aa68dd2202b55b1f8",
            "cf7a7ecc1f0c4a789b2d2fc90fbac899",
            "5da91444d2304464be2e27779c225bb1",
            "7d113f543cea4f1a8ad38b46cb79a722",
            "11f1656e4de74a3db408231e5a3bbe0a",
            "075f15a2e4324bb790890a54aa0595b2",
            "7207e108228246eabf129843690321ea",
            "416d6f2c41d84b2891d30db67747b4a9",
            "c4cbbd26da39440f8bc2a1ad4c70c8de",
            "8d0f7927cec142e5bd862efb109607e7",
            "8919d59178e3444c837694d667fca42c",
            "05b693bb947e4bde8f453709092ab718",
            "7dec31957a6345ac8485c6806d184c5f",
            "e197f0ccb1214dfb96802863d2bc3995",
            "20f32fb58e4d4d698a07af8275313e68",
            "110952bd874c439087dfc737a31025d3",
            "f640e90f319b4d45bd5b1a4e6e6430c0",
            "d95e8722d2b944c0bb00332cf7c69773",
            "f198618eb17543e29bc4caf893228ff2",
            "a6af8fc7a83143f28fcdb97a97aebc91",
            "83b62e3121234b8c851d4c15296c54bc"
          ]
        },
        "id": "wLZlkfaF7OGs",
        "outputId": "29700e56-eb0a-4219-8595-3c8f116b2831"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "304f7423f0f3423289886e185f789c80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbc47e20a0704c00a2bdde345fc037e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "openai_humaneval/test-00000-of-00001.par(…):   0%|          | 0.00/83.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8919d59178e3444c837694d667fca42c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds = load_dataset(\"openai/openai_humaneval\")\n",
        "humaneval = ds[\"test\"].to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rJjKWMsM7dc8"
      },
      "outputs": [],
      "source": [
        "selected_indices = [0, 1, 2, 3, 4, 9, 11, 14, 25, 39]\n",
        "selected_problems = humaneval.iloc[selected_indices].reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNoc-3h-7hqa",
        "outputId": "e7503d75-1be7-4afa-bae2-daaadfa8e817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 10 problems\n",
            "Problem IDs: ['HumanEval/0', 'HumanEval/1', 'HumanEval/2', 'HumanEval/3', 'HumanEval/4', 'HumanEval/9', 'HumanEval/11', 'HumanEval/14', 'HumanEval/25', 'HumanEval/39']\n"
          ]
        }
      ],
      "source": [
        "print(\"Problem IDs:\", selected_problems['task_id'].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jv6MqYcH7mre"
      },
      "outputs": [],
      "source": [
        "class PromptStrategies:\n",
        "    \"\"\"Six different prompting strategies as required\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def chain_of_thought(prompt):\n",
        "        return f\"\"\"Let's think about this step by step.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "First, I'll understand the requirements, then write the solution.\n",
        "Let me work through this methodically:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def stepwise_cot(prompt):\n",
        "        return f\"\"\"I'll solve this problem following these explicit steps:\n",
        "Step 1: Parse the function signature and understand inputs/outputs\n",
        "Step 2: Analyze the provided examples\n",
        "Step 3: Identify edge cases to handle\n",
        "Step 4: Write the implementation\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Following each step:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_planning(prompt):\n",
        "        return f\"\"\"I need to create a plan before implementing this.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "My implementation plan:\n",
        "1. Data structures needed:\n",
        "2. Algorithm approach:\n",
        "3. Edge cases to handle:\n",
        "\n",
        "Now implementing based on this plan:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_debugging(prompt):\n",
        "        return f\"\"\"I'll write this solution and then check it for bugs.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Initial implementation (then I'll review for bugs):\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_edit(prompt):\n",
        "        return f\"\"\"I'll implement this focusing on clean, efficient code.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Here's my optimized solution:\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_repair(prompt):\n",
        "        return f\"\"\"I'll write a robust solution with proper error handling.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "Robust implementation with error handling:\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9T2nNNcX7xSh"
      },
      "outputs": [],
      "source": [
        "def call_openai(prompt):\n",
        "    \"\"\"Call openai model (Model Family 1: Mixture of Experts)\"\"\"\n",
        "    try:\n",
        "        completion = groq_client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write clean, correct code.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=800,\n",
        "            top_p=1,\n",
        "            stream=False\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"openai error: {e}\")\n",
        "        time.sleep(5)\n",
        "        return \"\"\n",
        "\n",
        "def call_llama(prompt):\n",
        "    \"\"\"Call Llama model (Model Family 2: Meta's LLaMA)\"\"\"\n",
        "    try:\n",
        "        completion = groq_client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert Python programmer. Write clean, correct code.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=800,\n",
        "            top_p=1,\n",
        "            stream=False\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Llama error: {e}\")\n",
        "        time.sleep(5)\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "w2qB5wlm719H"
      },
      "outputs": [],
      "source": [
        "def extract_code(text):\n",
        "    \"\"\"Extract Python code from model response\"\"\"\n",
        "\n",
        "    if \"```python\" in text:\n",
        "        pattern = r'```python\\n(.*?)```'\n",
        "        matches = re.findall(pattern, text, re.DOTALL)\n",
        "        if matches:\n",
        "            return matches[-1].strip()\n",
        "\n",
        "    if \"```\" in text:\n",
        "        pattern = r'```\\n(.*?)```'\n",
        "        matches = re.findall(pattern, text, re.DOTALL)\n",
        "        if matches:\n",
        "            return matches[-1].strip()\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    code_lines = []\n",
        "    in_func = False\n",
        "    base_indent = 0\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip().startswith('def '):\n",
        "            in_func = True\n",
        "            base_indent = len(line) - len(line.lstrip())\n",
        "            code_lines = [line]\n",
        "        elif in_func:\n",
        "            if line.strip() == '':\n",
        "                code_lines.append(line)\n",
        "            elif len(line) - len(line.lstrip()) > base_indent:\n",
        "                code_lines.append(line)\n",
        "            elif line.strip() and not line[0].isspace():\n",
        "                break\n",
        "            else:\n",
        "                code_lines.append(line)\n",
        "\n",
        "    if code_lines:\n",
        "        return '\\n'.join(code_lines)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def test_solution(code, test_code, entry_point):\n",
        "    \"\"\"Test generated code against test cases\"\"\"\n",
        "    try:\n",
        "        namespace = {}\n",
        "\n",
        "        exec(code, namespace)\n",
        "\n",
        "        exec(test_code, namespace)\n",
        "\n",
        "        if 'check' in namespace and entry_point in namespace:\n",
        "            try:\n",
        "                namespace['check'](namespace[entry_point])\n",
        "                return True, \"All tests passed\"\n",
        "            except AssertionError as e:\n",
        "                return False, f\"Test failed: {str(e)}\"\n",
        "            except Exception as e:\n",
        "                return False, f\"Runtime error: {str(e)}\"\n",
        "        else:\n",
        "            return False, \"Function not found in generated code\"\n",
        "\n",
        "    except SyntaxError as e:\n",
        "        return False, f\"Syntax error: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Execution error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJJ4nfyN8CZG",
        "outputId": "1d23f890-7599-49a2-f252-7f741174356c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Part 1 Experiment...\n",
            "\n",
            "============================================================\n",
            "PART 1: Prompt Design & Code Generation\n",
            "Testing 6 strategies × 2 model families × 10 problems\n",
            "============================================================\n",
            "\n",
            "Problem 1/10: HumanEval/0\n",
            "  [1/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [2/120] CoT + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [3/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [4/120] SCoT + Llama3-70B: Fail\n",
            "  [5/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [6/120] Self-Plan + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [7/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [8/120] Self-Debug + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [9/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [10/120] Self-Edit + Llama3-70B: Pass\n",
            "  [11/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [12/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 2/10: HumanEval/1\n",
            "  [13/120] CoT + openai/gpt-oss-20b: ↻Fail\n",
            "  [14/120] CoT + Llama3-70B: Fail\n",
            "  [15/120] SCoT + openai/gpt-oss-20b: Fail\n",
            "  [16/120] SCoT + Llama3-70B: ['()', '(())', '(()())']\n",
            "Pass\n",
            "  [17/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [18/120] Self-Plan + Llama3-70B: ['()', '(())', '(()())']\n",
            "Pass\n",
            "  [19/120] Self-Debug + openai/gpt-oss-20b: Fail\n",
            "  [20/120] Self-Debug + Llama3-70B: ['()', '(())', '(()())']\n",
            "Pass\n",
            "  [21/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [22/120] Self-Edit + Llama3-70B: ['()', '(())', '(()())']\n",
            "Pass\n",
            "  [23/120] Self-Repair + openai/gpt-oss-20b: ↻↻Fail\n",
            "  [24/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 3/10: HumanEval/2\n",
            "  [25/120] CoT + openai/gpt-oss-20b: Fail\n",
            "  [26/120] CoT + Llama3-70B: Pass\n",
            "  [27/120] SCoT + openai/gpt-oss-20b: Fail\n",
            "  [28/120] SCoT + Llama3-70B: Fail\n",
            "  [29/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [30/120] Self-Plan + Llama3-70B: 0.5\n",
            "Pass\n",
            "  [31/120] Self-Debug + openai/gpt-oss-20b: Fail\n",
            "  [32/120] Self-Debug + Llama3-70B: Fail\n",
            "  [33/120] Self-Edit + openai/gpt-oss-20b: ↻Fail\n",
            "  [34/120] Self-Edit + Llama3-70B: Fail\n",
            "  [35/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [36/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 4/10: HumanEval/3\n",
            "  [37/120] CoT + openai/gpt-oss-20b: Fail\n",
            "  [38/120] CoT + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [39/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [40/120] SCoT + Llama3-70B: Fail\n",
            "  [41/120] Self-Plan + openai/gpt-oss-20b: Pass\n",
            "  [42/120] Self-Plan + Llama3-70B: False\n",
            "True\n",
            "False\n",
            "False\n",
            "True\n",
            "Pass\n",
            "  [43/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [44/120] Self-Debug + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [45/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [46/120] Self-Edit + Llama3-70B: False\n",
            "True\n",
            "Pass\n",
            "  [47/120] Self-Repair + openai/gpt-oss-20b: Pass\n",
            "  [48/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 5/10: HumanEval/4\n",
            "  [49/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [50/120] CoT + Llama3-70B: 1.0\n",
            "Pass\n",
            "  [51/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [52/120] SCoT + Llama3-70B: Fail\n",
            "  [53/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [54/120] Self-Plan + Llama3-70B: 1.0\n",
            "Pass\n",
            "  [55/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [56/120] Self-Debug + Llama3-70B: 1.0\n",
            "Pass\n",
            "  [57/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [58/120] Self-Edit + Llama3-70B: 1.0\n",
            "Pass\n",
            "  [59/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [60/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 6/10: HumanEval/9\n",
            "  [61/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [62/120] CoT + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [63/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [64/120] SCoT + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [65/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [66/120] Self-Plan + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [67/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [68/120] Self-Debug + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [69/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [70/120] Self-Edit + Llama3-70B: [1, 2, 3, 3, 3, 4, 4]\n",
            "Pass\n",
            "  [71/120] Self-Repair + openai/gpt-oss-20b: Pass\n",
            "  [72/120] Self-Repair + Llama3-70B: Fail\n",
            "\n",
            "Problem 7/10: HumanEval/11\n",
            "  [73/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [74/120] CoT + Llama3-70B: Pass\n",
            "  [75/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [76/120] SCoT + Llama3-70B: 100\n",
            "Pass\n",
            "  [77/120] Self-Plan + openai/gpt-oss-20b: Pass\n",
            "  [78/120] Self-Plan + Llama3-70B: 100\n",
            "Pass\n",
            "  [79/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [80/120] Self-Debug + Llama3-70B: 100\n",
            "Pass\n",
            "  [81/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [82/120] Self-Edit + Llama3-70B: Pass\n",
            "  [83/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [84/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 8/10: HumanEval/14\n",
            "  [85/120] CoT + openai/gpt-oss-20b: Pass\n",
            "  [86/120] CoT + Llama3-70B: ['a', 'ab', 'abc']\n",
            "Pass\n",
            "  [87/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [88/120] SCoT + Llama3-70B: ['a', 'ab', 'abc']\n",
            "[]\n",
            "['a']\n",
            "Pass\n",
            "  [89/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [90/120] Self-Plan + Llama3-70B: ['a', 'ab', 'abc']\n",
            "[]\n",
            "['a']\n",
            "Pass\n",
            "  [91/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [92/120] Self-Debug + Llama3-70B: Pass\n",
            "  [93/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [94/120] Self-Edit + Llama3-70B: Pass\n",
            "  [95/120] Self-Repair + openai/gpt-oss-20b: ↻Pass\n",
            "  [96/120] Self-Repair + Llama3-70B: Fail\n",
            "\n",
            "Problem 9/10: HumanEval/25\n",
            "  [97/120] CoT + openai/gpt-oss-20b: Fail\n",
            "  [98/120] CoT + Llama3-70B: [2, 2, 2]\n",
            "[5, 5]\n",
            "[2, 5, 7]\n",
            "Pass\n",
            "  [99/120] SCoT + openai/gpt-oss-20b: Pass\n",
            "  [100/120] SCoT + Llama3-70B: [2, 2, 2]\n",
            "[5, 5]\n",
            "[2, 5, 7]\n",
            "Pass\n",
            "  [101/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [102/120] Self-Plan + Llama3-70B: [2, 2, 2]\n",
            "[5, 5]\n",
            "[2, 5, 7]\n",
            "Pass\n",
            "  [103/120] Self-Debug + openai/gpt-oss-20b: Pass\n",
            "  [104/120] Self-Debug + Llama3-70B: Fail\n",
            "  [105/120] Self-Edit + openai/gpt-oss-20b: Pass\n",
            "  [106/120] Self-Edit + Llama3-70B: [2, 2, 2]\n",
            "[5, 5]\n",
            "[2, 5, 7]\n",
            "Pass\n",
            "  [107/120] Self-Repair + openai/gpt-oss-20b: Fail\n",
            "  [108/120] Self-Repair + Llama3-70B: Pass\n",
            "\n",
            "Problem 10/10: HumanEval/39\n",
            "  [109/120] CoT + openai/gpt-oss-20b: Fail\n",
            "  [110/120] CoT + Llama3-70B: Fail\n",
            "  [111/120] SCoT + openai/gpt-oss-20b: Fail\n",
            "  [112/120] SCoT + Llama3-70B: Pass\n",
            "  [113/120] Self-Plan + openai/gpt-oss-20b: Fail\n",
            "  [114/120] Self-Plan + Llama3-70B: Pass\n",
            "  [115/120] Self-Debug + openai/gpt-oss-20b: Fail\n",
            "  [116/120] Self-Debug + Llama3-70B: 2\n",
            "3\n",
            "5\n",
            "13\n",
            "89\n",
            "Pass\n",
            "  [117/120] Self-Edit + openai/gpt-oss-20b: Fail\n",
            "  [118/120] Self-Edit + Llama3-70B: Pass\n",
            "  [119/120] Self-Repair + openai/gpt-oss-20b: ↻↻Fail\n",
            "  [120/120] Self-Repair + Llama3-70B: Pass\n"
          ]
        }
      ],
      "source": [
        "def run_part1_experiment():\n",
        "    \"\"\"Part 1: Test prompting strategies (8 points)\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PART 1: Prompt Design & Code Generation\")\n",
        "    print(\"Testing 6 strategies × 2 model families × 10 problems\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    strategies = {\n",
        "        \"CoT\": PromptStrategies.chain_of_thought,\n",
        "        \"SCoT\": PromptStrategies.stepwise_cot,\n",
        "        \"Self-Plan\": PromptStrategies.self_planning,\n",
        "        \"Self-Debug\": PromptStrategies.self_debugging,\n",
        "        \"Self-Edit\": PromptStrategies.self_edit,\n",
        "        \"Self-Repair\": PromptStrategies.self_repair\n",
        "    }\n",
        "\n",
        "    models = {\n",
        "        \"openai/gpt-oss-20b\": call_openai,\n",
        "        \"Llama3-70B\": call_llama\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    total_tests = len(selected_problems) * len(strategies) * len(models)\n",
        "    current_test = 0\n",
        "\n",
        "    for prob_idx, problem in selected_problems.iterrows():\n",
        "        print(f\"\\nProblem {prob_idx+1}/10: {problem['task_id']}\")\n",
        "\n",
        "        for strat_name, strat_func in strategies.items():\n",
        "            for model_name, model_func in models.items():\n",
        "                current_test += 1\n",
        "                print(f\"  [{current_test}/{total_tests}] {strat_name} + {model_name}: \", end=\"\", flush=True)\n",
        "\n",
        "                prompt = strat_func(problem['prompt'])\n",
        "\n",
        "                response = \"\"\n",
        "                for attempt in range(3):\n",
        "                    response = model_func(prompt)\n",
        "                    if response:\n",
        "                        break\n",
        "                    print(\"↻\", end=\"\", flush=True)\n",
        "                    time.sleep(3)\n",
        "\n",
        "                code = extract_code(response)\n",
        "                success, error = test_solution(code, problem['test'], problem['entry_point'])\n",
        "\n",
        "                results.append({\n",
        "                    'problem_id': problem['task_id'],\n",
        "                    'problem_idx': prob_idx,\n",
        "                    'strategy': strat_name,\n",
        "                    'model': model_name,\n",
        "                    'success': success,\n",
        "                    'error': error if not success else None,\n",
        "                    'prompt_used': prompt[:300] + \"...\" if len(prompt) > 300 else prompt,\n",
        "                    'response': response[:500] + \"...\" if len(response) > 500 else response,\n",
        "                    'generated_code': code\n",
        "                })\n",
        "\n",
        "                print(\"Pass\" if success else \"Fail\")\n",
        "\n",
        "                time.sleep(2.5)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"Starting Part 1 Experiment...\")\n",
        "part1_results = run_part1_experiment()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16ju75gW8IAr",
        "outputId": "d0898bd9-f056-4174-f9f1-009ed3955a69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PART 1: RESULTS ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Overall Success Rate: 67.5%\n",
            "   Successful: 81/120\n",
            "\n",
            "Success Rate by Strategy:\n",
            "   Self-Edit: 85.0% (17/20 passed)\n",
            "   Self-Debug: 75.0% (15/20 passed)\n",
            "   SCoT: 65.0% (13/20 passed)\n",
            "   CoT: 65.0% (13/20 passed)\n",
            "   Self-Plan: 60.0% (12/20 passed)\n",
            "   Self-Repair: 55.0% (11/20 passed)\n",
            "\n",
            "Success Rate by Model Family:\n",
            "   Llama3-70B: 81.7% (49/60 passed)\n",
            "   openai/gpt-oss-20b: 53.3% (32/60 passed)\n",
            "\n",
            "Strategy × Model Success Rate (%):\n",
            "model        Llama3-70B  openai/gpt-oss-20b  Average\n",
            "strategy                                            \n",
            "CoT                80.0                50.0     65.0\n",
            "SCoT               60.0                70.0     65.0\n",
            "Self-Debug         80.0                70.0     75.0\n",
            "Self-Edit          90.0                80.0     85.0\n",
            "Self-Plan         100.0                20.0     60.0\n",
            "Self-Repair        80.0                30.0     55.0\n",
            "\n",
            "Top 3 Strategy-Model Combinations:\n",
            "   Self-Plan + Llama3-70B: 100.0%\n",
            "   Self-Edit + Llama3-70B: 90.0%\n",
            "   CoT + Llama3-70B: 80.0%\n",
            "\n",
            "Results saved to part1_results.csv\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PART 1: RESULTS ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "overall_success = part1_results['success'].mean()\n",
        "print(f\"\\nOverall Success Rate: {overall_success*100:.1f}%\")\n",
        "print(f\"   Successful: {part1_results['success'].sum()}/{len(part1_results)}\")\n",
        "\n",
        "print(\"\\nSuccess Rate by Strategy:\")\n",
        "strategy_stats = part1_results.groupby('strategy')['success'].agg(['mean', 'sum', 'count'])\n",
        "strategy_stats['percentage'] = strategy_stats['mean'] * 100\n",
        "strategy_stats = strategy_stats.sort_values('percentage', ascending=False)\n",
        "for idx, row in strategy_stats.iterrows():\n",
        "    print(f\"   {idx}: {row['percentage']:.1f}% ({int(row['sum'])}/{int(row['count'])} passed)\")\n",
        "\n",
        "print(\"\\nSuccess Rate by Model Family:\")\n",
        "model_stats = part1_results.groupby('model')['success'].agg(['mean', 'sum', 'count'])\n",
        "model_stats['percentage'] = model_stats['mean'] * 100\n",
        "for idx, row in model_stats.iterrows():\n",
        "    print(f\"   {idx}: {row['percentage']:.1f}% ({int(row['sum'])}/{int(row['count'])} passed)\")\n",
        "\n",
        "print(\"\\nStrategy × Model Success Rate (%):\")\n",
        "cross_tab = part1_results.pivot_table(\n",
        "    values='success',\n",
        "    index='strategy',\n",
        "    columns='model',\n",
        "    aggfunc='mean'\n",
        ") * 100\n",
        "cross_tab['Average'] = cross_tab.mean(axis=1)\n",
        "print(cross_tab.round(1))\n",
        "\n",
        "print(\"\\nTop 3 Strategy-Model Combinations:\")\n",
        "combo_stats = part1_results.groupby(['strategy', 'model'])['success'].mean().sort_values(ascending=False).head(3)\n",
        "for (strat, model), rate in combo_stats.items():\n",
        "    print(f\"   {strat} + {model}: {rate*100:.1f}%\")\n",
        "\n",
        "part1_results.to_csv('part1_results.csv', index=False)\n",
        "print(\"\\nResults saved to part1_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnwzKU8l8wEk",
        "outputId": "db5a5547-0d81-476b-8fc0-fcec5fcd6d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PART 2: Debugging & Iterative Improvement\n",
            "============================================================\n",
            "\n",
            "Debugging Case 1:\n",
            "   Problem: HumanEval/1\n",
            "   Original: CoT + Llama3-70B\n",
            "   Error: Execution error: name 'List' is not defined...\n",
            "   Attempting fix with openai... Fixed!\n",
            "   Analysis: The model successfully identified and fixed the issue.\n",
            "\n",
            "Debugging Case 2:\n",
            "   Problem: HumanEval/1\n",
            "   Original: CoT + openai/gpt-oss-20b\n",
            "   Error: Syntax error: expected an indented block after 'else' statement on line 49 (<string>, line 50)...\n",
            "   Attempting fix with openai... Still has issues\n",
            "   Analysis: Debugging attempt improved but didn't fully resolve the issue.\n",
            "   Remaining error: Test failed: ...\n"
          ]
        }
      ],
      "source": [
        "def run_part2_debugging():\n",
        "    \"\"\"Part 2: Debug failed cases\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PART 2: Debugging & Iterative Improvement\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    failed = part1_results[~part1_results['success']]\n",
        "\n",
        "    if len(failed) < 2:\n",
        "        print(\"Less than 2 failed cases available!\")\n",
        "        failed = part1_results.head(2)\n",
        "    else:\n",
        "        failed = failed.groupby(['strategy', 'model']).first().head(2).reset_index()\n",
        "\n",
        "    debug_results = []\n",
        "\n",
        "    for idx in range(min(2, len(failed))):\n",
        "        fail = failed.iloc[idx]\n",
        "        print(f\"\\nDebugging Case {idx+1}:\")\n",
        "        print(f\"   Problem: {fail['problem_id']}\")\n",
        "        print(f\"   Original: {fail['strategy']} + {fail['model']}\")\n",
        "        print(f\"   Error: {fail['error'][:100]}...\")\n",
        "\n",
        "        prob = selected_problems[selected_problems['task_id'] == fail['problem_id']].iloc[0]\n",
        "\n",
        "        debug_prompt = f\"\"\"You are debugging Python code. Fix the following error.\n",
        "\n",
        "PROBLEM DESCRIPTION:\n",
        "{prob['prompt']}\n",
        "\n",
        "FAILED CODE:\n",
        "```python\n",
        "{fail['generated_code']}\n",
        "```\n",
        "\n",
        "ERROR MESSAGE:\n",
        "{fail['error']}\n",
        "\n",
        "Please provide a CORRECTED version that will pass all test cases.\n",
        "Think about what went wrong and fix it.\n",
        "\n",
        "CORRECTED CODE:\"\"\"\n",
        "\n",
        "        print(\"   Attempting fix with openai...\", end=\"\", flush=True)\n",
        "\n",
        "        fixed_response = call_openai(debug_prompt)\n",
        "        fixed_code = extract_code(fixed_response)\n",
        "\n",
        "        success, error = test_solution(fixed_code, prob['test'], prob['entry_point'])\n",
        "\n",
        "        debug_results.append({\n",
        "            'problem_id': fail['problem_id'],\n",
        "            'original_strategy': fail['strategy'],\n",
        "            'original_model': fail['model'],\n",
        "            'original_error': fail['error'],\n",
        "            'debug_prompt': debug_prompt[:500] + \"...\",\n",
        "            'fixed_code': fixed_code,\n",
        "            'debug_success': success,\n",
        "            'debug_error': error if not success else None,\n",
        "            'improvement': \"Fixed\" if success else \"Still failing\"\n",
        "        })\n",
        "\n",
        "        print(f\" {'Fixed!' if success else 'Still has issues'}\")\n",
        "\n",
        "        if success:\n",
        "            print(f\"   Analysis: The model successfully identified and fixed the issue.\")\n",
        "        else:\n",
        "            print(f\"   Analysis: Debugging attempt improved but didn't fully resolve the issue.\")\n",
        "            print(f\"   Remaining error: {error[:100]}...\")\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "    return debug_results\n",
        "\n",
        "part2_results = run_part2_debugging()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRv_n28f806u",
        "outputId": "1925693d-f72e-4367-945e-6c7bd2c60781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Debugging Summary:\n",
            "   Success rate: 50% (1/2)\n",
            "\n",
            "   HumanEval/1:\n",
            "   - Original: CoT + Llama3-70B\n",
            "   - Result: Fixed\n",
            "\n",
            "   HumanEval/1:\n",
            "   - Original: CoT + openai/gpt-oss-20b\n",
            "   - Result: Still failing\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nDebugging Summary:\")\n",
        "if part2_results:\n",
        "    debug_success_rate = sum(r['debug_success'] for r in part2_results) / len(part2_results)\n",
        "    print(f\"   Success rate: {debug_success_rate*100:.0f}% ({sum(r['debug_success'] for r in part2_results)}/{len(part2_results)})\")\n",
        "\n",
        "    for r in part2_results:\n",
        "        print(f\"\\n   {r['problem_id']}:\")\n",
        "        print(f\"   - Original: {r['original_strategy']} + {r['original_model']}\")\n",
        "        print(f\"   - Result: {r['improvement']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB9G2Kfd89XP",
        "outputId": "466e395f-f518-43ee-b137-2d3c3e175cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PART 3: Innovation - Test-Driven Incremental Generation\n",
            "============================================================\n",
            "\n",
            "Innovation Strategy: Test-Driven Incremental Generation\n",
            "   - Shows concrete test examples upfront\n",
            "   - Iteratively refines based on test failures\n",
            "   - Maximum 3 iterations per problem\n",
            "\n",
            "Testing innovative strategy on selected problems:\n",
            "\n",
            "Problem: HumanEval/0\n",
            "   openai/gpt-oss-20b (openai family):\n",
            "      Iteration 1: Pass Success!\n",
            "   Llama3-70B (LLaMA family):\n",
            "      Iteration 1: Pass Success!\n",
            "Problem: HumanEval/1\n",
            "   openai/gpt-oss-20b (openai family):\n",
            "      Iteration 1: Fail      Iteration 2: Fail      Iteration 3: Fail Max iterations reached\n",
            "   Llama3-70B (LLaMA family):\n",
            "      Iteration 1: Pass Success!\n",
            "\n",
            "Comparative Analysis:\n",
            "\n",
            "Performance Comparison:\n",
            "      Baseline strategies (avg): 62.5%\n",
            "      TDIG Innovation strategy: 75.0%\n",
            "      Relative improvement: +12.5%\n",
            "\n",
            "Efficiency Analysis:\n",
            "      Average iterations needed: 1.5\n",
            "      Success on first try: 2/4\n"
          ]
        }
      ],
      "source": [
        "def run_part3_innovation():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PART 3: Innovation - Test-Driven Incremental Generation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\nInnovation Strategy: Test-Driven Incremental Generation\")\n",
        "    print(\"   - Shows concrete test examples upfront\")\n",
        "    print(\"   - Iteratively refines based on test failures\")\n",
        "    print(\"   - Maximum 3 iterations per problem\")\n",
        "\n",
        "    def test_driven_generation(problem):\n",
        "        test_lines = []\n",
        "        for line in problem['test'].split('\\n'):\n",
        "            if 'assert candidate' in line:\n",
        "                test_lines.append(line.strip())\n",
        "                if len(test_lines) >= 2:\n",
        "                    break\n",
        "\n",
        "        prompt = f\"\"\"Look at these specific test cases that your function must pass:\n",
        "\n",
        "{chr(10).join(test_lines)}\n",
        "\n",
        "Now implement this function to pass these tests:\n",
        "\n",
        "{problem['prompt']}\n",
        "\n",
        "Focus on making sure your implementation handles these exact test cases correctly.\n",
        "Write the complete function:\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def iterative_refinement(problem, model_func, model_name, max_iterations=3):\n",
        "\n",
        "        iteration_results = []\n",
        "        current_prompt = test_driven_generation(problem)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            print(f\"      Iteration {iteration+1}: \", end=\"\", flush=True)\n",
        "\n",
        "            response = model_func(current_prompt)\n",
        "            code = extract_code(response)\n",
        "\n",
        "            success, error = test_solution(code, problem['test'], problem['entry_point'])\n",
        "\n",
        "            iteration_results.append({\n",
        "                'iteration': iteration + 1,\n",
        "                'success': success,\n",
        "                'error': error if not success else None\n",
        "            })\n",
        "\n",
        "            print(\"Pass\" if success else \"Fail\", end=\"\", flush=True)\n",
        "\n",
        "            if success:\n",
        "                print(f\" Success!\")\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'iterations': iteration + 1,\n",
        "                    'final_code': code,\n",
        "                    'iteration_details': iteration_results\n",
        "                }\n",
        "\n",
        "            current_prompt = f\"\"\"The previous code failed with this error:\n",
        "{error}\n",
        "\n",
        "Failed code:\n",
        "```python\n",
        "{code}\n",
        "```\n",
        "\n",
        "Fix this specific error and make the code pass all tests.\n",
        "Remember the tests that must pass:\n",
        "{chr(10).join([line.strip() for line in problem['test'].split(chr(10)) if 'assert candidate' in line][:2])}\n",
        "\n",
        "Corrected code:\"\"\"\n",
        "\n",
        "            time.sleep(3)\n",
        "\n",
        "        print(f\" Max iterations reached\")\n",
        "        return {\n",
        "            'success': False,\n",
        "            'iterations': max_iterations,\n",
        "            'final_code': code,\n",
        "            'iteration_details': iteration_results\n",
        "        }\n",
        "\n",
        "    test_problems = selected_problems.head(2)\n",
        "    innovation_results = []\n",
        "\n",
        "    print(\"\\nTesting innovative strategy on selected problems:\\n\")\n",
        "\n",
        "    for idx, prob in test_problems.iterrows():\n",
        "        print(f\"Problem: {prob['task_id']}\")\n",
        "\n",
        "        print(f\"   openai/gpt-oss-20b (openai family):\")\n",
        "        mixtral_result = iterative_refinement(prob, call_openai, \"openai\")\n",
        "\n",
        "        print(f\"   Llama3-70B (LLaMA family):\")\n",
        "        llama_result = iterative_refinement(prob, call_llama, \"Llama\")\n",
        "\n",
        "        innovation_results.append({\n",
        "            'problem': prob['task_id'],\n",
        "            'mixtral_success': mixtral_result['success'],\n",
        "            'mixtral_iterations': mixtral_result['iterations'],\n",
        "            'llama_success': llama_result['success'],\n",
        "            'llama_iterations': llama_result['iterations']\n",
        "        })\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "    print(\"\\nComparative Analysis:\")\n",
        "\n",
        "    baseline_subset = part1_results[part1_results['problem_id'].isin(test_problems['task_id'])]\n",
        "    baseline_rate = baseline_subset['success'].mean()\n",
        "\n",
        "    total_innovation_tests = len(innovation_results) * 2\n",
        "    innovation_successes = sum(r['mixtral_success'] + r['llama_success'] for r in innovation_results)\n",
        "    innovation_rate = innovation_successes / total_innovation_tests if total_innovation_tests > 0 else 0\n",
        "\n",
        "    print(f\"\\nPerformance Comparison:\")\n",
        "    print(f\"      Baseline strategies (avg): {baseline_rate*100:.1f}%\")\n",
        "    print(f\"      TDIG Innovation strategy: {innovation_rate*100:.1f}%\")\n",
        "    improvement = (innovation_rate - baseline_rate) * 100\n",
        "    print(f\"      Relative improvement: {'+' if improvement >= 0 else ''}{improvement:.1f}%\")\n",
        "\n",
        "    print(f\"\\nEfficiency Analysis:\")\n",
        "    avg_iterations = sum(r['mixtral_iterations'] + r['llama_iterations'] for r in innovation_results) / (len(innovation_results) * 2)\n",
        "    print(f\"      Average iterations needed: {avg_iterations:.1f}\")\n",
        "    print(f\"      Success on first try: {sum(1 for r in innovation_results if r['mixtral_iterations'] == 1 or r['llama_iterations'] == 1)}/{total_innovation_tests}\")\n",
        "\n",
        "    return innovation_results\n",
        "\n",
        "part3_results = run_part3_innovation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPYNBxab9DG8",
        "outputId": "76cf0a0b-e38d-47e1-963a-c462736261c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Final Summary\n",
            "============================================================\n",
            "\n",
            "Key Results:\n",
            "   • Best Strategy: Self-Edit (85.0%)\n",
            "   • Best Model: Llama3-70B (81.7%)\n",
            "   • Overall Success Rate: 67.5%\n",
            "   • Debugging Success: 50%\n",
            "   • Innovation Strategy: 75%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Final Summary\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "print(\"\\nKey Results:\")\n",
        "best_strategy = part1_results.groupby('strategy')['success'].mean().idxmax()\n",
        "best_model = part1_results.groupby('model')['success'].mean().idxmax()\n",
        "overall_rate = part1_results['success'].mean()\n",
        "\n",
        "print(f\"   • Best Strategy: {best_strategy} ({part1_results[part1_results['strategy']==best_strategy]['success'].mean()*100:.1f}%)\")\n",
        "print(f\"   • Best Model: {best_model} ({part1_results[part1_results['model']==best_model]['success'].mean()*100:.1f}%)\")\n",
        "print(f\"   • Overall Success Rate: {overall_rate*100:.1f}%\")\n",
        "\n",
        "if part2_results:\n",
        "    debug_rate = sum(r['debug_success'] for r in part2_results) / len(part2_results)\n",
        "    print(f\"   • Debugging Success: {debug_rate*100:.0f}%\")\n",
        "\n",
        "if part3_results:\n",
        "    innovation_success = sum(r['mixtral_success'] + r['llama_success'] for r in part3_results) / (len(part3_results) * 2)\n",
        "    print(f\"   • Innovation Strategy: {innovation_success*100:.0f}%\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05b693bb947e4bde8f453709092ab718": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_110952bd874c439087dfc737a31025d3",
            "placeholder": "​",
            "style": "IPY_MODEL_f640e90f319b4d45bd5b1a4e6e6430c0",
            "value": "Generating test split: 100%"
          }
        },
        "075f15a2e4324bb790890a54aa0595b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10bb747a1f4f4a11a448cdfea2ae3709": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "110952bd874c439087dfc737a31025d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f1656e4de74a3db408231e5a3bbe0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b476a7339043afa9f46073346919d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ecdc64d79d9439c8f69e0d8a197a6f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f32fb58e4d4d698a07af8275313e68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "298d610d241e4d4aa68dd2202b55b1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11f1656e4de74a3db408231e5a3bbe0a",
            "placeholder": "​",
            "style": "IPY_MODEL_075f15a2e4324bb790890a54aa0595b2",
            "value": "openai_humaneval/test-00000-of-00001.par(…): 100%"
          }
        },
        "304f7423f0f3423289886e185f789c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_745c8c4199584d6db45742f2df5e3354",
              "IPY_MODEL_f71de3ebaa7d4ad1b08da5bf4c4aee0c",
              "IPY_MODEL_91c1ad2e26194be49f425cb654b1223e"
            ],
            "layout": "IPY_MODEL_1ecdc64d79d9439c8f69e0d8a197a6f0"
          }
        },
        "416d6f2c41d84b2891d30db67747b4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5902ead2046a43daa2690679a0b125ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5da91444d2304464be2e27779c225bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4cbbd26da39440f8bc2a1ad4c70c8de",
            "placeholder": "​",
            "style": "IPY_MODEL_8d0f7927cec142e5bd862efb109607e7",
            "value": " 83.9k/83.9k [00:00&lt;00:00, 171kB/s]"
          }
        },
        "5ef82c01953e426ba68d41835ff72aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7207e108228246eabf129843690321ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "745c8c4199584d6db45742f2df5e3354": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f563213891414898392c48e761c8c0",
            "placeholder": "​",
            "style": "IPY_MODEL_19b476a7339043afa9f46073346919d3",
            "value": "README.md: "
          }
        },
        "7d113f543cea4f1a8ad38b46cb79a722": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dec31957a6345ac8485c6806d184c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d95e8722d2b944c0bb00332cf7c69773",
            "max": 164,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f198618eb17543e29bc4caf893228ff2",
            "value": 164
          }
        },
        "83b62e3121234b8c851d4c15296c54bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8919d59178e3444c837694d667fca42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05b693bb947e4bde8f453709092ab718",
              "IPY_MODEL_7dec31957a6345ac8485c6806d184c5f",
              "IPY_MODEL_e197f0ccb1214dfb96802863d2bc3995"
            ],
            "layout": "IPY_MODEL_20f32fb58e4d4d698a07af8275313e68"
          }
        },
        "8d0f7927cec142e5bd862efb109607e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "905ed85b9b4c483b9068aa39a7b7cd38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91c1ad2e26194be49f425cb654b1223e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5902ead2046a43daa2690679a0b125ca",
            "placeholder": "​",
            "style": "IPY_MODEL_10bb747a1f4f4a11a448cdfea2ae3709",
            "value": " 6.52k/? [00:00&lt;00:00, 128kB/s]"
          }
        },
        "a6af8fc7a83143f28fcdb97a97aebc91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4cbbd26da39440f8bc2a1ad4c70c8de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbc47e20a0704c00a2bdde345fc037e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_298d610d241e4d4aa68dd2202b55b1f8",
              "IPY_MODEL_cf7a7ecc1f0c4a789b2d2fc90fbac899",
              "IPY_MODEL_5da91444d2304464be2e27779c225bb1"
            ],
            "layout": "IPY_MODEL_7d113f543cea4f1a8ad38b46cb79a722"
          }
        },
        "cf7a7ecc1f0c4a789b2d2fc90fbac899": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7207e108228246eabf129843690321ea",
            "max": 83920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_416d6f2c41d84b2891d30db67747b4a9",
            "value": 83920
          }
        },
        "d95e8722d2b944c0bb00332cf7c69773": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e197f0ccb1214dfb96802863d2bc3995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6af8fc7a83143f28fcdb97a97aebc91",
            "placeholder": "​",
            "style": "IPY_MODEL_83b62e3121234b8c851d4c15296c54bc",
            "value": " 164/164 [00:00&lt;00:00, 3282.63 examples/s]"
          }
        },
        "e3f563213891414898392c48e761c8c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f198618eb17543e29bc4caf893228ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f640e90f319b4d45bd5b1a4e6e6430c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f71de3ebaa7d4ad1b08da5bf4c4aee0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ef82c01953e426ba68d41835ff72aeb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_905ed85b9b4c483b9068aa39a7b7cd38",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
